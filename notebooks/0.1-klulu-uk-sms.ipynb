{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.config\n",
    "%aimport src.helpers\n",
    "%aimport src.transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import iterparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import dill\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from functools import partial\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout, Activation, Input, Embedding, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers, Model, Sequential, callbacks, optimizers, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import data_dir, models_dir\n",
    "from src.helpers import calc_metrics, plot_tfidf_classfeats_h, top_feats_by_class, init_dir, save_model, load_model, print_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process raw SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"karim-sms-allow.xml\"\n",
    "source = data_dir / filename\n",
    "data = []\n",
    "for event, elem in iterparse(source):\n",
    "    if elem.tag == \"sms\":\n",
    "        #if any(elem.attrib[\"body\"]==r[\"text\"] for r in data):\n",
    "        #    continue\n",
    "        record = {}\n",
    "        record[\"text\"] = elem.attrib[\"body\"]\n",
    "        record[\"contact_name\"] = elem.attrib[\"contact_name\"]\n",
    "        record[\"address\"] = elem.attrib[\"address\"]\n",
    "        record[\"timestamp\"] = int(elem.attrib[\"date\"])\n",
    "        record[\"type\"] = elem.attrib[\"type\"]\n",
    "        data.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_excel(data_dir / \"karim-sms-allow.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(filenames, file_out, date_format=\"%m-%d-%Y %H:%M:%S\", is_save=1):\n",
    "    output = []\n",
    "    for k,v in filenames.items():\n",
    "        if k == \"labeled\":\n",
    "            df = pd.read_excel(data_dir / v, sheet_name=\"total sms\")\n",
    "            df[\"timestamp\"] = (df[\"timestamp\"] / 1000).map(datetime.fromtimestamp)\n",
    "            df[\"resp\"] = 0\n",
    "            df[\"source\"] = \"K\"\n",
    "            output.append(df)\n",
    "        elif k == \"labeled_1\":\n",
    "            df = pd.read_excel(data_dir / v)\n",
    "            df[\"resp\"] = 0\n",
    "            df[\"timestamp\"] = df[\"timestamp\"].map(lambda x: datetime.strptime(x, date_format))\n",
    "            exclude = [\"Karimushka\"]\n",
    "            df = df.loc[~(df.contact_name.isin(exclude))]\n",
    "            df[\"source\"] = \"T\"\n",
    "            output.append(df)\n",
    "        else:\n",
    "            df = pd.read_excel(data_dir / v)\n",
    "            df = df.rename(columns={\"SMS text\": \"text\", \n",
    "                                    \"Is it a spam or ham?\": \"label\",\n",
    "                                    \"Timestamp\": \"timestamp\"})\n",
    "            df[\"resp\"] = 1\n",
    "            df[\"label\"] = df[\"label\"].map(lambda x: LABEL_MAP.get(x, x))\n",
    "            output.append(df)\n",
    "    df = pd.concat(output, ignore_index=True)\n",
    "    if is_save:\n",
    "        df.to_excel(data_dir / file_out)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"ham\": 0, \"spam\": 1}\n",
    "FILES = {\"labeled\": \"karim-sms-allow-labeled.xlsx\",\n",
    "         \"labeled_1\": \"tanya-sms-all.xlsx\",\n",
    "         \"responses\": \"SMS Data Collection (Responses).xlsx\"}\n",
    "file_out = \"sms-uk-total.xlsx\"\n",
    "total = build_dataset(FILES, file_out=file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6104, 9)\n",
      "0.0    80.079\n",
      "1.0    19.921\n",
      "Name: label, dtype: float64\n",
      "0\n",
      "(6104, 9)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensionality and class imbalance\n",
    "print(total.shape)\n",
    "print(total.label.value_counts(normalize=True).round(5)*100)\n",
    "print(total.text.isnull().sum())\n",
    "total = total.loc[total.text.notnull()]\n",
    "print(total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.loc[total.resp==1, \"label\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_excel(data_dir / file_out)\n",
    "total = total.loc[total.text.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total[\"text_rep\"] = total[\"text\"].str.replace(r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\", \"PHONE_NUMBER\", flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total[\"text\"] = total[\"text\"].str.replace(r\"[\\n\\r]+\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of train: 4272, Num. of test: 1831\n"
     ]
    }
   ],
   "source": [
    "X = total[\"text\"]\n",
    "y = total[\"label\"]\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "print(f\"Num. of train: {len(X_train)}, Num. of test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5855"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.shape[0] - 249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(X_train, X_test, var=\"text\", features=None, vectorizer=None):\n",
    "    f_train = []\n",
    "    f_test = []\n",
    "    for feature in features:\n",
    "        if feature == \"tfidf\":\n",
    "            tf_train = vectorizer.fit_transform(X_train).toarray()\n",
    "            tf_test = vectorizer.transform(X_test).toarray()\n",
    "            f_train.append(tf_train)\n",
    "            f_test.append(tf_test)\n",
    "        if feature == \"length\":\n",
    "            if \"tfidf\" in features:\n",
    "                train = (tf_train>0).sum(axis=1)[:, np.newaxis]\n",
    "                test = (tf_test>0).sum(axis=1)[:, np.newaxis]\n",
    "            else:\n",
    "                train = X_train.map(len).values[:, np.newaxis]\n",
    "                test = X_test.map(len).values[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        if feature == \"patt\":\n",
    "            patt = \"%|taxi|скидк|цін\"\n",
    "            train = (X_train.str.contains(patt, regex=True, flags=re.I)\n",
    "                     .astype(int).values[:, np.newaxis])\n",
    "            test = (X_test.str.contains(patt, regex=True, flags=re.I)\n",
    "                    .astype(int).values[:, np.newaxis])\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        if feature == \"phone\":\n",
    "            patt = r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\"\n",
    "            train = X_train.map(lambda x: len(re.findall(patt, x))>0).values[:, np.newaxis]\n",
    "            test = X_test.map(lambda x: len(re.findall(patt, x))>0).values[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "    return np.concatenate((f_train), axis=1), np.concatenate((f_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_params = {\"lowercase\": True,\n",
    "             \"analyzer\": \"char_wb\",\n",
    "             \"stop_words\": None,\n",
    "             \"ngram_range\": (4, 4),\n",
    "             \"min_df\": 0.0,\n",
    "             \"max_df\": 1.0,\n",
    "             \"preprocessor\": None,#Preprocessor(),\n",
    "             \"max_features\": 3500,\n",
    "             \"norm\": \"l2\"*0,\n",
    "             \"use_idf\": 1\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Top N features\n",
    "# top = 100\n",
    "# r = tfidf_train.toarray().sum(axis=1)\n",
    "# topn_ids = np.argsort(r)[::-1][:top]\n",
    "# voc = [f for i,f in enumerate(features) if i not in topn_ids]\n",
    "# tf_params[\"vocabulary\"] = None#voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**tf_params)\n",
    "tfidf_train = vectorizer.fit_transform(X_train)\n",
    "tfidf_test = vectorizer.transform(X_test)\n",
    "features = [\n",
    "            \"tfidf\", \n",
    "            \"length\",\n",
    "            \"phone\",\n",
    "            \"patt\",\n",
    "]\n",
    "train, test = build_features(X_train, X_test, features=features, vectorizer=vectorizer, var=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-8\n",
    "input_tfidf = Input(shape=(train.shape[1],))\n",
    "x = Dense(100, #activation=activations.tanh,\n",
    "         kernel_regularizer=regularizers.l2(alpha),\n",
    "         use_bias=1\n",
    "         )(input_tfidf)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = Dense(50, activation=activations.tanh,\n",
    "#          kernel_regularizer=regularizers.l2(alpha))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "output = Dense(1, activation=\"sigmoid\",\n",
    "               use_bias=1,\n",
    "               kernel_regularizer=regularizers.l2(alpha)\n",
    "              )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        (None, 3503)              0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 100)               350400    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 350,501\n",
      "Trainable params: 350,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_tfidf, outputs=output)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSprop(lr=0.01), \n",
    "              metrics=[f1])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(y_train),\n",
    "                                             y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4272 samples, validate on 1831 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4711 - f1: 0.8179 - val_loss: 0.3757 - val_f1: 0.8624\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1343 - f1: 0.9512 - val_loss: 0.1672 - val_f1: 0.9434\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.0780 - f1: 0.9725 - val_loss: 0.1774 - val_f1: 0.9431\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0574 - f1: 0.9823 - val_loss: 0.2194 - val_f1: 0.9421\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0825 - f1: 0.9803 - val_loss: 0.2189 - val_f1: 0.9356\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0641 - f1: 0.9871 - val_loss: 0.2840 - val_f1: 0.9213\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0482 - f1: 0.9850 - val_loss: 0.2186 - val_f1: 0.9459\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0650 - f1: 0.9850 - val_loss: 0.2280 - val_f1: 0.9399\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0421 - f1: 0.9905 - val_loss: 0.2466 - val_f1: 0.9415\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0337 - f1: 0.9899 - val_loss: 0.2237 - val_f1: 0.9405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6a67eecf8>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, y_train, validation_data=(test, y_test), \n",
    "          epochs=10, \n",
    "          batch_size=64,\n",
    "          class_weight=weights,\n",
    "          verbose=2,\n",
    "          shuffle=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9395218002812941"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9765155652648826"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9175824175824175"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.962536023054755"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = model.predict(test)\n",
    "y_pred = np.zeros_like(probas)\n",
    "y_pred[probas>=0.5] = 1\n",
    "metrics.f1_score(y_pred=y_pred, y_true=y_test)\n",
    "metrics.accuracy_score(y_pred, y_test)\n",
    "metrics.precision_score(y_pred, y_test)\n",
    "metrics.recall_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = vectorizer.get_feature_names()\n",
    "# dfs = top_feats_by_class(tfidf_train, y_train, features, min_tfidf=0.1, top_n=25)\n",
    "# plot_tfidf_classfeats_h(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is much worse to misclassify ham\n",
    "SMS than letting spam pass the filter. So, it is desirable to be able to bias\n",
    "the filter towards classifying SMS as ham, yielding higher precision at the expense of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tf, X_test, clf, w=1.5):\n",
    "    probas = clf.predict_proba(X_test)\n",
    "    ratios = np.log(probas[:, 1] ) - np.log(probas[:, 0])\n",
    "    lengths = (tf.toarray()>0).sum(axis=1).T\n",
    "    thresholds = lengths * np.log(w)\n",
    "    y_pred = np.zeros_like(y_test)\n",
    "    y_pred[ratios>thresholds] = 1\n",
    "    return y_pred, ratios, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(min_samples_leaf=5, min_samples_split=15,\n",
    "                             n_estimators=100, max_depth=20, max_features=\"auto\", \n",
    "                             class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.02, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=25,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.997\n",
      "Recall: 0.967\n",
      "Precision: 0.962\n",
      "F1: 0.964\n",
      "Accuracy: 0.986\n",
      "\n",
      "Confusion matrix:\n",
      "      pred_ham  pred_spam\n",
      "ham       1453         14\n",
      "spam        12        352\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      1467\n",
      "          1       0.96      0.97      0.96       364\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=25, class_weight=\"balanced\", \n",
    "                         C=0.02, penalty=\"l2\")\n",
    "#clf = MultinomialNB(alpha=0.01)#, class_prior=[0.5, 0.5])\n",
    "clf.fit(train, y_train)\n",
    "#pred, ratios, thresholds = predict_class(tfidf_test, test, clf, w=1.2)\n",
    "pred = clf.predict(test)\n",
    "proba = clf.predict_proba(test)[:, 1]\n",
    "output, report, conf_matrix = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"], \n",
    "                                           print_=True, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[fn_i[:2]]\n",
    "total.loc[3469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_i = np.where((pred==1) & (y_test==0))[0]\n",
    "fn_i = np.where((pred==0) & (y_test==1))[0]\n",
    "for el in X_test.iloc[fp_i].values:\n",
    "    print(el+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformers import TfIdfLen, ModelTransformer, MatchPattern, Length, Converter\n",
    "from src.pipeline import grid_search, analyze_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x88 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 88 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = X_test.iloc[:1]#.values\n",
    "l = TfIdfLen(add_len=1, **tf_params)\n",
    "l.fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tf = {#\"union__vec__vec__use_idf\": [0, 1],\n",
    "           #\"union__vec__vec__ngram_range\": [(3,3), (4,4), (5,5), (3,5), (3,4)],\n",
    "           #\"union__vec__vec__max_features\": range(2000, 4500, 500)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypertuning model 1 out of 2: logit\n",
      "================================================================================\n",
      "Best score on training set (CV): 0.955\n",
      "Best parameters set:\n",
      "0.9552 (+/-0.0022) for {'logit__C': 0.1}: [0.95454545 0.95       0.9519833  0.96296296 0.95634096]\n",
      "0.9547 (+/-0.0035) for {'logit__C': 0.2}: [0.95670103 0.94560669 0.9519833  0.96694215 0.95218295]\n",
      "0.9547 (+/-0.0035) for {'logit__C': 0.3}: [0.95670103 0.94560669 0.9519833  0.96694215 0.95218295]\n",
      "0.9546 (+/-0.0039) for {'logit__C': 0.4}: [0.95867769 0.94339623 0.9519833  0.96694215 0.95218295]\n",
      "0.9550 (+/-0.0036) for {'logit__C': 0.5}: [0.95867769 0.94537815 0.9519833  0.96694215 0.95218295]\n",
      "0.9533 (+/-0.0042) for {'logit__C': 1}: [0.95652174 0.94291755 0.9539749  0.96694215 0.94605809]\n",
      "0.9499 (+/-0.0044) for {'logit__C': 5}: [0.95435685 0.93842887 0.9519833  0.9626556  0.94190871]\n",
      "0.9507 (+/-0.0050) for {'logit__C': 10}: [0.95238095 0.93842887 0.9539749  0.96680498 0.94190871]\n",
      "Hypertuning model 2 out of 2: nb\n",
      "================================================================================\n",
      "Best score on training set (CV): 0.852\n",
      "Best parameters set:\n",
      "0.8518 (+/-0.0102) for {'nb__alpha': 0.1}: [0.83450704 0.84135472 0.86131387 0.8342246  0.88764045]\n",
      "0.8497 (+/-0.0099) for {'nb__alpha': 0.5}: [0.83362522 0.83392226 0.86181818 0.83597884 0.88311688]\n",
      "0.8451 (+/-0.0104) for {'nb__alpha': 1}: [0.82926829 0.82578397 0.86025408 0.83157895 0.87867647]\n",
      "0.8276 (+/-0.0113) for {'nb__alpha': 5}: [0.81367521 0.81646655 0.83597884 0.80405405 0.86799277]\n",
      "0.8158 (+/-0.0123) for {'nb__alpha': 10}: [0.79932546 0.80541455 0.82495667 0.79008264 0.85918004]\n"
     ]
    }
   ],
   "source": [
    "best_estimators, best_scores = grid_search(transformer_grid=grid_tf, tf_params=tf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, results, conf_matrix, fnp = analyze_model(model=best_estimators[0], datafile=file_out, log_fold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sms = \"привіт заходь до нас у ввечері додому\"\n",
    "# ham, spam = pipe.predict_proba(sms)[0]\n",
    "# print(f\"Probability ham: {ham*100:0.3f}%\\nProbability spam: {spam*100:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}