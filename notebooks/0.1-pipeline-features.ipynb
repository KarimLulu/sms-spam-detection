{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.transformers\n",
    "%aimport src.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import iterparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer, word_tokenize\n",
    "import dill\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from functools import partial\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import data_dir, models_dir\n",
    "from src.helpers import (calc_metrics, plot_tfidf_classfeats_h, top_feats_by_class, \n",
    "                         init_dir, save_model, load_model, print_dict)\n",
    "from src.transformers import (TfIdfLen, ModelTransformer, MatchPattern, Length, \n",
    "                              Converter, Transformer, unsquash, Select)\n",
    "from src.pipeline import (grid_search, analyze_model, load_data, build_transform_pipe, TF_PARAMS, PATTERNS,\n",
    "                          get_vec_pipe, get_pattern_pipe, TOKEN_FEATURES, build_all_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data[\"text\"].map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of train: 4272, Num. of test: 1831\n"
     ]
    }
   ],
   "source": [
    "X = data[[\"text\", \"tokens\"]]\n",
    "y = data[\"label\"]\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "print(f\"Num. of train: {len(X_train)}, Num. of test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRUBER_URLINTEXT_PAT = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\n",
    "                                  (?:[^\\s()<>]|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+\n",
    "                                  (?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>\n",
    "                                  ?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))\"\"\", re.X)\n",
    "WEB_URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.]\n",
    "                (?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro\n",
    "                |tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh\n",
    "                |bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy\n",
    "                |cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi\n",
    "                |gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo\n",
    "                |jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk\n",
    "                |ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe\n",
    "                |pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl\n",
    "                |sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug\n",
    "                |uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\n",
    "                \\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|\n",
    "                [^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.]\n",
    "                (?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post\n",
    "                |pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|\n",
    "                bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co\n",
    "                |cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga\n",
    "                |gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in\n",
    "                |io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu\n",
    "                |lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng\n",
    "                |ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa\n",
    "                |sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk\n",
    "                |tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za\n",
    "                |zm|zw)\\b/?(?!@)))\"\"\"\n",
    "CURRENCY_PATT = u\"[$¬¢¬£¬§¬•÷èÿã‡ß≤‡ß≥‡ßª‡´±‡Øπ‡∏ø·üõ\\u20a0-\\u20bd\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_params = {'lowercase': True,\n",
    " 'analyzer': 'char_wb',\n",
    " 'stop_words': None,\n",
    " 'ngram_range': (4, 4),\n",
    " 'min_df': 0.0,\n",
    " 'max_df': 1.0,\n",
    " 'preprocessor': None,\n",
    " 'max_features': 4000,\n",
    " 'norm': '',\n",
    " 'use_idf': 1}\n",
    "patterns = [(r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\", {\"name\": \"phone\",\n",
    "                                            \"is_len\": 0}),\n",
    "           (r\"%|taxi|—Å–∫–∏–¥(?:–∫|–æ—á–Ω)|—Ü[—ñ–µ]–Ω|–∑–Ω–∏–∂–∫|—Ç–∞–∫—Å[–∏—ñ]|–ø—Ä–æ–º–æ|–∞–∫—Ü[—ñ–∏]|bonus|–±–æ–Ω—É—Å\", {\"name\": \"custom\",\n",
    "                                  \"is_len\": 0,\n",
    "                                  \"flags\": re.I | re.U}),\n",
    "#            (r\"[+-<>/^]\", {\"name\": \"math_ops\", \"is_len\": 0}),\n",
    "            (r\"[.]\", {\"name\": \"dot\", \"is_len\": 0}),\n",
    "#            (WEB_URL_REGEX, {\"name\": \"url\", \"is_len\": 0, \"flags\": re.X}),\n",
    "            (CURRENCY_PATT, {\"name\": \"currency\", \"is_len\": 0, \"flags\": re.U}),\n",
    "#            (r\"[*]\", {\"name\": \"special_symbols\", \"is_len\": 0})\n",
    "            (r\":\\)|:\\(|-_-|:p|:v|:\\*|:o|B-\\)|:‚Äô\\(\", {\"name\": \"emoji\", \"is_len\": 0, \"flags\": re.U}),\n",
    "            (r\"[0-9]{2,4}[.-/][0-9]{2,4}[.-/][0-9]{2,4}\", {\"name\": \"date\", \"is_len\": 0})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_pipe(tokenizer=word_tokenize, features=TOKEN_FEATURES):\n",
    "    token_features = TokenFeatures(tokenizer, features=features)\n",
    "    tok_pipe = [\n",
    "        (\"selector\", Select([\"tokens\"], to_np=0)),\n",
    "        ('tok', token_features)]\n",
    "    return Pipeline(tok_pipe)\n",
    "\n",
    "def get_vec_pipe(add_len=True, tfidf_params={}):\n",
    "    vectorizer = TfIdfLen(add_len, **tfidf_params)\n",
    "    vec_pipe = [\n",
    "        ('vec', vectorizer)]\n",
    "    return Pipeline(vec_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_pipe(patterns):\n",
    "    pipes = []\n",
    "    for i, (patt, params) in enumerate(patterns):\n",
    "        kwargs = params.copy()\n",
    "        name = kwargs.pop(\"name\") + \"_\" + str(i)\n",
    "        transformer = MatchPattern(pattern=patt, **kwargs)\n",
    "        pipes.append((name, transformer))\n",
    "    return pipes\n",
    "\n",
    "def get_len_pipe(use_tfidf=True, vec_pipe=None):\n",
    "    len_pipe = [(\"length\", Length(use_tfidf))]\n",
    "    if use_tfidf:\n",
    "        len_pipe.insert(0, (\"vec\", vec_pipe))\n",
    "    return Pipeline(len_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform_pipe(tf_params=tf_params, add_len=True, vec_mode=\"add\", patterns=patterns,\n",
    "                         tokenizer=word_tokenize, features=TOKEN_FEATURES):\n",
    "    vec_pipe = get_vec_pipe(add_len, tf_params)\n",
    "    if vec_mode == \"only\":\n",
    "        return vec_pipe\n",
    "    patt_pipe = get_pattern_pipe(patterns)\n",
    "    chain = [\n",
    "        ('selector', Select([\"text\"], to_np=0)),\n",
    "        ('converter', Converter()),\n",
    "        ('union', FeatureUnion([\n",
    "            ('vec', vec_pipe),\n",
    "            *patt_pipe\n",
    "        ]))\n",
    "    ]\n",
    "    tok_pipe = get_tokens_pipe(tokenizer, features)\n",
    "    final_chain = FeatureUnion([(\"chain\", Pipeline(chain)),\n",
    "                                (\"tok\", tok_pipe)])\n",
    "    return [(\"final_chain\", final_chain)]\n",
    "\n",
    "def build_classifier(name, seed=25):\n",
    "    if name == \"logit\":\n",
    "        model = LogisticRegression(C=1, class_weight=\"balanced\", random_state=seed, penalty=\"l2\")\n",
    "        model.grid_s = {f'{name}__C' : (0.1, 0.2, 0.3, 0.4, 0.5, 1, 5, 10)}\n",
    "        model.grid_b = {f'{name}__C' : [(1)]}\n",
    "    elif name == \"nb\":\n",
    "        model = MultinomialNB(alpha=0.1) #class_prior=[0.5, 0.5])\n",
    "        model.grid_s = {f'{name}__alpha' : (0.1, 0.5, 1, 5, 10)}\n",
    "        model.grid_b = {f'{name}__alpha' : [(1)]}\n",
    "    model.name = name\n",
    "    return model\n",
    "\n",
    "def get_estimator_pipe(name, model, tf_params, vec_mode=\"add\", patterns=patterns, features=TOKEN_FEATURES):\n",
    "    chain = build_transform_pipe(tf_params, vec_mode=vec_mode, patterns=patterns, features=features)\n",
    "    chain.append((name, model))\n",
    "    pipe = Pipeline(chain)\n",
    "    pipe.name = name\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1831x4007 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89252 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_pipe = get_vec_pipe(True, tf_params)\n",
    "patt_pipe = get_pattern_pipe(patterns)\n",
    "chain = [\n",
    "    ('selector', Select([\"text\"], to_np=0)),\n",
    "      ('converter', Converter()),\n",
    "    ('union', FeatureUnion([\n",
    "        ('vec', vec_pipe),\n",
    "        *patt_pipe\n",
    "   ]))\n",
    "]\n",
    "pipe = Pipeline(chain)\n",
    "pipe.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lower(tokens):\n",
    "    return any(token.islower() for token in tokens)\n",
    "\n",
    "def is_upper(tokens):\n",
    "    return any(token.isupper() for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenFeatures(Transformer):\n",
    "\n",
    "    def __init__(self, tokenizer=word_tokenize, features=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.features = features\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict()\n",
    "\n",
    "    def _get_features(self, tokens):\n",
    "        output = []\n",
    "        for f in self.features:\n",
    "            output.append(eval(f)(tokens))\n",
    "        return np.array(output)\n",
    "    \n",
    "    def _job(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "        return self._get_features(tokens)\n",
    "        \n",
    "    def transform(self, X, **kwargs):  \n",
    "        rez = []\n",
    "        for record in X:\n",
    "            temp = self._get_features(record)\n",
    "            rez.append(temp)\n",
    "        return np.array(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = build_transform_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('final_chain', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('chain', Pipeline(memory=None,\n",
       "     steps=[('selector', <src.transformers.Select object at 0x7f7c265a4278>), ('converter', <src.transformers.Converter object at 0x7f7c266157b8>), ('union', FeatureUnion(n_jobs=1,\n",
       "       transform...alty='l2', random_state=25,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = build_classifier(\"logit\")\n",
    "pipe = get_estimator_pipe(clf.name, clf, tf_params)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability ham: 41.728%\n",
      "Probability spam: 58.272%\n"
     ]
    }
   ],
   "source": [
    "sms = \"–ø–æ—ó–¥–µ–º –¥–æ –Ω–∏—Ö –Ω–∞ —Ç–∞–∫—Å—ñ?\"\n",
    "sms_df = pd.DataFrame({\"text\": sms, \"tokens\": word_tokenize(sms)})\n",
    "ham, spam = pipe.predict_proba(sms_df)[0]\n",
    "print(f\"Probability ham: {ham*100:0.3f}%\\nProbability spam: {spam*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'[0-9]{2,4}[.-/\\\\\\\\][0-9]{2,4}[.-/\\\\\\\\][0-9]{2,4}'\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['21.04.2016']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = r\"[0-9]{2,4}[.-/][0-9]{2,4}[.-/][0-9]{2,4}\"\n",
    "repr(p)\n",
    "re.findall(p, \"21.04.2016\", re.U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypertuning model 1 out of 1: logit\n",
      "================================================================================\n",
      "Best score on training set (CV): 0.960\n",
      "Best parameters set:\n",
      "0.9587 (+/-0.0029) for {'logit__C': 0.1}: [0.9694501  0.95652174 0.95218295 0.95867769 0.95687885]\n",
      "0.9599 (+/-0.0041) for {'logit__C': 0.2}: [0.97352342 0.95652174 0.95       0.96465696 0.95473251]\n",
      "0.9590 (+/-0.0041) for {'logit__C': 0.3}: [0.97142857 0.95652174 0.94780793 0.96465696 0.95473251]\n",
      "0.9573 (+/-0.0037) for {'logit__C': 0.4}: [0.96734694 0.95652174 0.94560669 0.9625     0.95473251]\n",
      "0.9569 (+/-0.0034) for {'logit__C': 0.5}: [0.96523517 0.95652174 0.94560669 0.9625     0.95473251]\n",
      "0.9556 (+/-0.0035) for {'logit__C': 1}: [0.96523517 0.95652174 0.94560669 0.96033403 0.95041322]\n",
      "0.9548 (+/-0.0037) for {'logit__C': 5}: [0.96326531 0.95850622 0.94560669 0.96049896 0.94605809]\n",
      "0.9527 (+/-0.0032) for {'logit__C': 10}: [0.95705521 0.95850622 0.94560669 0.95833333 0.94409938]\n"
     ]
    }
   ],
   "source": [
    "best_estimators, best_scores = grid_search(patterns=patterns, estimator_names=[\"logit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'logit__C': 0.2},\n",
       "  'mean': 0.9598908445178538,\n",
       "  'scores': array([0.97352342, 0.95652174, 0.95      , 0.96465696, 0.95473251]),\n",
       "  'std': 0.008298807992172147}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'logit__C': 0.2},\n",
       "  'mean': 0.9599238561276642,\n",
       "  'scores': array([0.97154472, 0.95867769, 0.95      , 0.9625    , 0.95687885]),\n",
       "  'std': 0.007085036990868852}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall results\n",
      "AUC: 1.00 +/- 0.0011\n",
      "Accuracy: 0.98 +/- 0.0036\n",
      "F1: 0.96 +/- 0.0093\n",
      "Precision: 0.96 +/- 0.0074\n",
      "Recall: 0.96 +/- 0.0150\n",
      "\n",
      "Averaged confusion matrix\n",
      "      pred_ham  pred_spam\n",
      "ham      968.8        8.6\n",
      "spam      10.8      232.4\n",
      "\n",
      "Mean metrics\n",
      "accuracy: 0.984\n",
      "specificity: 0.964\n",
      "recall: 0.989\n",
      "precision: 0.991\n",
      "f1: 0.990\n"
     ]
    }
   ],
   "source": [
    "scores, results, conf_matrix, fnp = analyze_model(model=best_estimators[0], log_fold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tviy kod 90F7C416K. Obminyay yogo na Pepsi-Bonus v KFC Vokzal Pivdenniy z 16.04.2015 do 16.05.2015\n",
      "\n",
      "Z 04/07 centr VOLIA po vul.Kikvidze 1/2 bude zachineno na remont. Najblizhchi adresy dlya zvernen: vul.Vasylkivska 4, vul.Knazhyi Zaton 2/30\n",
      "\n",
      "Women's Day When: 7 March 22:00 Where: TAO Restaurant & Dance Bar 06735\n",
      "\n",
      "Lyubyi druzhe! Cherez nadzvychaino velyku kilkist otrymanyh lystiv - rozigrash pryziv vidbudetsya 27.05. Dyakuemo za uchast! Bazhaemo peremohy! hwclub.com.ua\n",
      "\n",
      "–ú–∞–≥–∞–∑–∏–Ω \"–ö–∞—Ä—Ñ—É—Ä\" —Å—Ç–∞–ª –±–æ–ª—å—à–µ, –Ω–æ–≤—ã–π —Ç–æ–≤–∞—Ä. –û—Ç–∫—Ä—ã—Ç–∏–µ 22.10 –≤ 11:00\n",
      "\n",
      "–í—Ä–µ–º—è –ø–æ–¥–∞—Ä–∫–æ–≤ –ø—Ä–∏—à–ª–æ! –¶–∏—Ç—Ä—É—Å –ø—Ä–æ–¥–ª–∏–ª —Ä–∞–±–æ—Ç—É –Ω–∞ —á–∞—Å! time.citrus.ua\n",
      "\n",
      "–ö–∏—ó–≤, –º–∏ –∑–Ω–∏–∑–∏–ª–∏ —Ü—ñ–Ω–∏ –Ω–∞ uberX! –í—ñ–¥—Ç–µ–ø–µ—Ä –ø–æ—ó–∑–¥–∫–∏ –ø–æ –º—ñ—Å—Ç—É - –≤—ñ–¥ 25 –≥—Ä–Ω\n",
      "\n",
      "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–π –Ω–∞–≥–æ–¥—É! –ù–∞ –≤–∞—à–æ–º—É —Ä–∞—Ö—É–Ω–∫—É 31 –±–æ–Ω—É—Å—ñ–≤. –í–∏—Ç—Ä–∞—á–∞–π –¥–æ 05.03!\n",
      "\n",
      "–ó–∞–∫–∞–∑‚Ññ4999 –æ–∂–∏–¥–∞–µ—Ç –í–∞—Å –ø–æ –∞–¥—Ä–µ—Å—É –ø—Ä.–õ–µ—Å—è–ö—É—Ä–±–∞—Å–∞,16-–∞ –¥–æ 19.03\n",
      "\n",
      "Tomorrow's forecast in SOMA South Park, San Francisco is Clear. https://m.twil.io/kYotCFy\n",
      "\n",
      "–ù–æ–≤—ã–µ –≥—Ä–∞—Ñ–∏–∫ –≤ –ö–ê–†–§–£–†!–ü—Ç-80%–ß—Ç-70%–°—Ä-60%–í—Ç-50%–ü–Ω-30%–í—Å-10%,–°–±-–ó–ê–í–û–ó!\n",
      "\n",
      "–ú–æ—Å–∫–∏—Ç–Ω—ã–µ —Å–µ—Ç–∫–∏ –†–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ –æ–∫–æ–Ω –†–µ–º–æ–Ω—Ç,—á–∏—Å—Ç–∫–∞ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä–∞ 0675136192\n",
      "\n",
      "–ü—Ä–∏–≤—ñ—Ç! –ù–∞–≥–∞–¥—É—î–º–æ, —â–æ –≤–∂–µ —Å—å–æ–≥–æ–¥–Ω—ñ –≤—ñ–¥–±—É–¥–µ—Ç—å—Å—è –ø–µ—Ä—à–µ –∑–∞–Ω—è—Ç—Ç—è –Ω–∞ –∫—É—Ä—Å—ñ Data Science. Natural Language Processing —É Projector (–≤—É–ª. –í–æ–∑–¥–≤–∏–∂–µ–Ω—Å—å–∫–∞, 34–ê). –ü–æ—á–∞—Ç–æ–∫ –æ 19:30. –ì–∞—Ä–Ω–æ–≥–æ –¥–Ω—è!\n",
      "\n",
      "Karim Maksimovich! Priobretaite bilety na poezd po cenam kassy vokzala kruglosutochno v Privat24 ili na bilet.pb.ua! pb.ua/1551\n",
      "\n",
      "–ü–æ—Å–ª—É–≥—É –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç –¢–∏–∂–¥–µ–Ω—å –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ!–í–∏ –∫–æ—Ä–∏—Å—Ç—É—î—Ç–µ—Å—å –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º –∑–∞ —Ç–∞—Ä–∏—Ñ–∞–º–∏ 15–≥—Ä–Ω –∑–∞ –∫–æ–∂–Ω—ñ 500–ú–ë. –¢–µ—Ä–º—ñ–Ω –¥—ñ—ó –ú–ë 7–¥–Ω—ñ–≤. –î–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–∞–ª–∏—à–∫—É –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–∞–∫–µ—Ç—É –Ω–∞–±–µ—Ä—ñ—Ç—å *121#.\n",
      "\n",
      "–ó 19.04.17 –∑–º—ñ–Ω—é—î—Ç—å—Å—è —Ç–∞—Ä–∏—Ñ –Ω–∞ —Ä–æ—É–º—ñ–Ω–≥ –≤ –ë—ñ–ª–æ—Ä—É—Å—ñ: –¥–∑–≤—ñ–Ω–∫–∏ 10—Ö–≤ - 45–≥—Ä–Ω, 100–ú–ë- 55–≥—Ä–Ω, 15 –≤–∏—Ö—ñ–¥–Ω–∏—Ö SMS- 20–≥—Ä–Ω. –ü–∞–∫–µ—Ç –¥–æ –∫—ñ–Ω—Ü—è –¥–æ–±–∏ (–∑–∞ –ö–∏—ó–≤—Å—å–∫–∏–º —á–∞—Å–æ–º), –Ω–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –≤ –∫—Ä–∞—ó–Ω—ñ. –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞–∫–µ—Ç—ñ–≤ –Ω–µ–æ–±–º–µ–∂–µ–Ω–∞. –î–µ—Ç–∞–ª—ñ: s.lifecell.ua/74 \n",
      "\n",
      "–®–∞–Ω–æ–≤–Ω–∏–π –∞–±–æ–Ω–µ–Ω—Ç–µ, –≤—ñ—Ç–∞—î–º–æ –í–∞—Å –∑ —Ç—Ä–∞–≤–Ω–µ–≤–∏–º–∏ –≤–∏—Ö—ñ–¥–Ω–∏–º–∏! –ë–∞–∂–∞—î–º–æ —Å–æ–Ω—è—á–Ω–æ–≥–æ —Ç–µ–ø–ª–∞ —ñ –ø—Ä–∏—î–º–Ω–æ–≥–æ —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è –∑ —Ä—ñ–¥–Ω–∏–º–∏ —Ç–∞ –¥—Ä—É–∑—è–º–∏ —â–æ–¥–Ω—è! –î—è–∫—É—î–º–æ –∑–∞ –¥–æ–≤—ñ—Ä—É, –í–∞—à lifecell\n",
      "\n",
      "–ü—Ä–∏–≤—ñ—Ç! UNFPA, –§–æ–Ω–¥ –Ω–∞—Ä–æ–¥–æ–Ω–∞—Å–µ–ª–µ–Ω–Ω—è –û–û–ù, —Ö–æ—á–µ –ø–æ—á—É—Ç–∏ –í–∞—à—É –¥—É–º–∫—É. –ß–æ–º—É –¥—ñ–≤—á–∞—Ç–∞ –º–µ–Ω—à–µ –æ—Ä—ñ—î–Ω—Ç–æ–≤–∞–Ω—ñ –Ω–∞ –∫–∞—Ä‚Äô—î—Ä—É –≤ –Ω–∞—É–∫–æ–≤–æ-—Ç–µ—Ö–Ω—ñ—á–Ω—ñ–π —Å—Ñ–µ—Ä—ñ? 1.–ù–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ —É —Å–≤–æ—ó—Ö —Å–∏–ª–∞—Ö —ñ —Ç–∞–ª–∞–Ω—Ç–∞—Ö 2.–£ —à–∫–æ–ª—ñ –ø–æ–≥–∞–Ω–æ –≤—á–∏–ª–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ –ø—Ä–µ–¥–º–µ—Ç–∏ 3.–í–≤–∞–∂–∞—é—Ç—å, —â–æ —Ä–æ–±–æ—Ç–∞ —É —Ü—ñ–π —Å—Ñ–µ—Ä—ñ –º–æ–∂–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ø–ª–∏–Ω—É—Ç–∏ –Ω–∞ –æ—Å–æ–±–∏—Å—Ç–µ –∂–∏—Ç—Ç—è 4.–ù–µ —Ü—ñ–∫–∞–≤–æ 5.–¶–µ —á–æ–ª–æ–≤—ñ—á–∞ —Å—Ñ–µ—Ä–∞ 6.–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –æ–±—ñ–∑–Ω–∞–Ω—ñ –ø—Ä–æ —Ü—é —Å—Ñ–µ—Ä—É 7.–†–∏–∑–∏–∫ –≤–∏—Å–æ–∫–æ—ó –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó 8.–Ü–Ω—à–µ(–≤–∫–∞–∂—ñ—Ç—å)\n",
      "\n",
      "–Ü –æ—Å—Ç–∞–Ω–Ω—î –∑–∞–ø–∏—Ç–∞–Ω–Ω—è: –Ø–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –í–∏ –±—É–¥–µ—Ç–µ  —Ä–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏ U-Report –≤–∞—à–∏–º –¥—Ä—É–∑—è–º? –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —à–∫–∞–ª—É –≤—ñ–¥ 0 –¥–æ 10, –¥–µ 0-—Ç–æ—á–Ω–æ –Ω–µ –±—É–¥—É —Ä–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏, –∞ 10 ‚Äì —Ç–æ—á–Ω–æ –±—É–¥—É —Ä–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏.\n",
      "\n",
      "–£–¥–∞—á–∞! –î–µ–Ω—å –ë–µ–ª—å—è –≤ Milavitsa 20-21.08! –¢—ã —É—Å–ø–µ–µ—à—å! –ò–Ω—Ñ–æ:0504470777\n",
      "\n",
      "Letnie skidki v TAC!- 50% na postelnoe belie Satin!Tolko 19-25 iyunia. ul.Baseynaya 5A tel.0668475131 DiscontInfo\n",
      "\n",
      "–ß–µ–∫–∞—î–º–æ –≤–∞—Å —Å—å–æ–≥–æ–¥–Ω—ñ –Ω–∞ MustHave fashion-–ª–µ–∫—Ü—ñ—ó –≤ Mozart Hotel –∑–∞ –∞–¥—Ä–µ—Å–æ—é –õ–∞–Ω–∂–µ—Ä–æ–Ω—ñ–≤—Å—å–∫–∞ 13. –û 15:00. –î–æ –∑—É—Å—Ç—Ä—ñ—á—ñ!\n",
      "\n",
      "–í–æ–Ω–∞ –∑–∞—á–µ–∫–∞–ª–∞—Å—è:) –¢–≤–æ—è –ø–µ—Ä—à–∞ –ï–ö–û-–∫–∞—Ä—Ç–∫–∞ –≤–∂–µ —É –≤—ñ–¥–¥—ñ–ª–µ–Ω–Ω—ñ –£–ö–†–ì–ê–ó–ë–ê–ù–ö–£ —É –º. –ö–∏—ó–≤ 0800309000\n",
      "\n",
      "–û—Å—Ç–∞–Ω–Ω—ñ 3 –¥–Ω—ñ! 2 –ø–∞—Ä–∏ –ª—ñ—Ç–∞ –∑–∞ 1399 –≥—Ä–Ω –¥–æ 17.09.17\n",
      "\n",
      "Kontrol kachestva Lamoda: esli u vas byla problema s zakazom, otpravte 1 v otvet, i my perezvonim vam, ili napishite nam na problema@lamoda.ua. Spasibo:-)\n",
      "\n",
      "–§–µ—Å—Ç–∏–≤–∞–ª—å —è–≥–Ω—è—Ç–∏–Ω—ã –≤ –ö—É–≤—à–∏–Ω–µ! –°–µ–º—å –≤–∫—É—Å–Ω–µ–π—à–∏—Ö –±–ª—é–¥ –∏–∑ –ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–≥–æ –∏ –¥–µ–ª–∏–∫–∞—Ç–µ—Å–Ω–æ–≥–æ –º—è—Å–∞! –ñ–¥–µ–º –≤–∞—Å –≤ –≥–æ—Å—Ç–∏!(067)4687258\n",
      "\n",
      "–ü—Ä–∏–≥–ª–∞—à–∞–µ–º –Ω–∞ –§–µ—Å—Ç–∏–≤–∞–ª—å —è–≥–Ω—è—Ç–∏–Ω—ã –≤ –ö—É–≤—à–∏–Ω–µ! –°–µ–º—å –≤–∫—É—Å–Ω–µ–π—à–∏—Ö –±–ª—é–¥ –∏–∑ –ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–≥–æ –∏ –¥–µ–ª–∏–∫–∞—Ç–µ—Å–Ω–æ–≥–æ –º—è—Å–∞! –ñ–¥–µ–º –≤–∞—Å –≤ –≥–æ—Å—Ç–∏!(067)4687258\n",
      "\n",
      "–§–µ—Å—Ç–∏–≤–∞–ª—å —Ö–∏–Ω–∫–∞–ª–∏ –≤ –ö—É–≤—à–∏–Ω–µ!–° –∏–Ω–¥–µ–π–∫–æ–π,—Ñ–æ—Ä–µ–ª—å—é,–≥—Ä–∏–±–∞–º–∏,–º—è—Å–æ–º –¥–∏–∫–æ–≥–æ –∫–∞–±–∞–Ω–∞,—à–ø–∏–Ω–∞—Ç–æ–º!–ñ–¥–µ–º –≤ –≥–æ—Å—Ç–∏!(067)4687258\n",
      "\n",
      "–ü—Ä–∏–≥–ª–∞—à–∞–µ–º –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã–µ –≤ \"–ö—É–≤—à–∏–Ω\"! –ù–æ–≤–æ–µ –º–µ–Ω—é –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω—ã—Ö –∫–∞–≤–∫–∞–∑—Å–∫–∏—Ö –±–ª—é–¥, –∂–∏–≤–∞—è –º—É–∑—ã–∫–∞, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è  –¥–µ—Ç—Å–∫–∞—è –∞–Ω–∏–º–∞—Ü–∏—è! –§—ë–¥–æ—Ä–æ–≤–∞ 10,0674687258\n",
      "\n",
      "–ù–æ–≤–∞—è —Ç—Ä–∞–¥–∏—Ü–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö-—É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≥–∏—Å—Å–∞—Ä—Å–∫–∏–π –±–∞—Ä–∞–Ω –≤ \"–ö—É–≤—à–∏–Ω–µ\"! –û—Å–æ–±–æ –Ω–µ–∂–Ω–∞—è –∏ –≤–∫—É—Å–Ω–∞—è –±–∞—Ä–∞–Ω–∏–Ω–∞!  –®—É—Ä–ø–∞, –∫–µ–±–∞–±, —à–∞—à–ª—ã–∫, —Å—Ç–µ–π–∫!–°–ø–µ—à–∏—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å! —É–ª.–§–µ–¥–æ—Ä–æ–≤–∞, 10.–¢–µ–ª. 0674687258.\n",
      "\n",
      "–ü—Ä–∏–≥–ª–∞—à–∞–µ–º –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã–µ –≤ \"–ö—É–≤—à–∏–Ω\"!–ë–ª—é–¥–∞ –∏–∑ –æ—Å–æ–±–æ–≥–æ –ì–∏—Å—Å–∞—Ä—Å–∫–æ–≥–æ –±–∞—Ä–∞–Ω–∞, —Ñ–∏—Ä–º–µ–Ω–Ω—ã–π –ø–ª–æ–≤ –æ—Ç –®–µ—Ñ–∞, –∞ –≤ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ-7 –≤–∏–¥–æ–≤ —Ö–∏–Ω–∫–∞–ª–∏!–ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞ –∏ –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ –∑–∞—Å—Ç–æ–ª—å—è! –ñ–¥–µ–º –≤ –≥–æ—Å—Ç–∏! 380674687258\n",
      "\n",
      "Z DNEM NARODZHENNYA! Budte z tymy, kogo liubyte! Robit te, u scho viryte! Zdorovia ta nathnennya! Robert Kossmann, zast. Golovy Pravlinnya Raiffeisen Bank Aval\n",
      "\n",
      "Shanovnyi Kliente, Vam dostupno 24750 UAH kredytnyh koshtiv vid Raiffeisen Banku Aval! Zavitayte do viddilennia! Info 0800500500\n",
      "\n",
      "–¢–µ–ø–µ—Ä —Ç–∞–∫ –ª–µ–≥–∫–æ —ñ –≤–∏–≥—ñ–¥–Ω–æ –Ω–∞–¥—ñ—Å–ª–∞—Ç–∏ –≥—Ä–æ—à—ñ —Å–≤–æ—ó–º —Ä—ñ–¥–Ω–∏–º! –ó–∞—Ö–æ–¥—å—Ç–µ https://paycell.lifecell.ua/transfer —Ç–∞ –ø–µ—Ä–µ–≤–æ–¥—å—Ç–µ –∫–æ—à—Ç–∏ –∑ –º–æ–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞ –±—É–¥—å-—è–∫—É –±–∞–Ω–∫—ñ–≤—Å—å–∫—É –∫–∞—Ä—Ç—É!\n",
      "\n",
      "Akcija v oktjabre! Francuzskoe narashhivanie nogtejj - 199 griven! Vash master Oksana zhdet Vas) 063 516 80 90. –õ—é—Ç–µ—Ä–∞–Ω—Å–∫–∞—è,3\n",
      "\n",
      "C–ª—É—Ö–∞–π—Ç–µ –ø—ñ—Å–Ω—ñ —É–ª—é–±–ª–µ–Ω–∏—Ö –≤–∏–∫–æ–Ω–∞–≤—Ü—ñ–≤ –±–µ–∑ –ø–µ—Ä–µ—à–∫–æ–¥ —Ç–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤—ñ–π —è–∫–æ—Å—Ç—ñ —É –º—É–∑–∏—á–Ω–æ–º—É –¥–æ–¥–∞—Ç–∫—É fizy! –ó–∞–≤–∞–Ω—Ç–∞–∂—É–π—Ç–µ http://fizy.com.ua/ob\n",
      "\n",
      "1-11 –º–∞—Ä—Ç–∞ 1+1=3 –Ω–∞ –±–µ–ª—å–µ –∏ –¥–æ–º.–æ–¥–µ–∂–¥—É  0674338487 hunkemoller.com.ua\n",
      "\n",
      "MGI –∑–∞–ø—Ä–æ—à—É—î –í–∞—Å –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç —Ö–æ—Ä—É —ñ–∑ –ê—Ä–∫–∞–Ω–∑–∞—Å—É —Å—å–æ–≥–æ–¥–Ω—ñ 26.04 –æ 19:00\n",
      "\n",
      "–í—Å—Ç—Ä–µ—á–∞–µ–º—Å—è —É–∂–µ –∑–∞–≤—Ç—Ä–∞ –≤ –Ω–æ–≤–æ–º –∫–ª—É–±–µ! L sektor —Å–º–µ–Ω–∏–ª –ø—Ä–æ–ø–∏—Å–∫—É - —É–ª. –Ø–º—Å–∫–∞—è 35/34 –ù–æ–≤—ã—Ö –ø–æ–±–µ–¥ –≤ 2018! –¢–≤–æ–π Lsektor.com\n",
      "\n",
      "–í–∞—à –±–æ—Ä–≥ –ø–µ—Ä–µ–¥ –¢–û–í –í–æ–ª—è –ö–∞–±–µ–ª—å –ø–µ—Ä–µ–¥–∞–Ω–æ –¥–æ CreditExpress.–£ –í–∞—Å 4 –≥–æ–¥–∏–Ω–∏ –¥–ª—è —Å–ø–ª–∞—Ç–∏ –±–æ—Ä–≥—É!‚Ññ –∫–≤–∏—Ç–∞–Ω—Ü—ñ—ó –ø–æ–≤—ñ–¥–æ–º—Ç–µ –∑–∞ —Ç–µ–ª 0444920550\n",
      "\n",
      "–Ø–∫—ñ—Å—Ç—å –∑–∞ —Å—É–ø–µ—Ä—Ü—ñ–Ω–æ—é! –û–ø—Ä–∞–≤–∏ BEST-150 –≥—Ä–Ω, EXO-200 –≥—Ä–Ω. luxoptica.ua\n",
      "\n",
      "6 –ª—ñ–Ω–∑ PureVision2+—Ä–æ–∑—á–∏–Ω Biotrue 60 ml = 654 –≥—Ä–Ω. –î–µ—Ç–∞–ª—ñ luxoptica.ua\n",
      "\n",
      "–û–±—ñ–¥–Ω—ñ–π —Å–µ—Ç –∑–∞ —Ü—ñ–Ω–æ—é –ø—ñ—Ü–∏: 79 –≥—Ä–Ω. –í —É—Å—ñ—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö –∑ 12:00 –¥–æ 15:00\n",
      "\n",
      "–ß—Ç–æ–±—ã –æ–±–Ω—è—Ç—å –±–ª–∏–∑–∫–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –¥–ª—è –Ω–∞—á–∞–ª–∞ –ª—É—á—à–µ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä—É–∫–∏:) –í–∞—à–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è —Å–∫–∏–¥–∫–∞ -14% –¥–æ 14-–≥–æ —Ñ–µ–≤—Ä–∞–ª—è –Ω–∞ bagman.ua\n",
      "\n",
      "–ë–µ–∑–º–µ–∂–Ω–µ —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è! 4 –ì–ë –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç—É —Ç–∞ 60 —Ö–≤. –Ω–∞ —ñ–Ω—à—ñ –Ω–æ–º–µ—Ä–∏ –ø–æ –£–∫—Ä–∞—ó–Ω—ñ –∑–∞ –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –Ω–∞ 70 –≥—Ä–Ω, 3 –ì–ë —Ç–∞ 40 —Ö–≤. –∑–∞ –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –Ω–∞ 50 –≥—Ä–Ω, 2 –ì–ë —Ç–∞ 20 —Ö–≤. –∑–∞ –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –Ω–∞ 30 –≥—Ä–Ω. –ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –æ–¥–Ω–∏–º –ø–ª–∞—Ç–µ–∂–µ–º –¥–æ 02.06.2018 (–≤–∫–ª—é—á–Ω–æ). –£–º–æ–≤–∏ –∑–∞ –Ω–æ–º–µ—Ä–æ–º 2153 (–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ)\n",
      "\n",
      "–§–ò–ù–ê–õ–¨–ù–ê–Ø –†–ê–°–ü–†–û–î–ê–ñ–ê ‚ö†Ô∏è   ‚ö†Ô∏è   ‚ö†Ô∏è –î–æ - 70% –Ω–∞ –±–æ–ª–µ–µ 1000 –∞—Ä–æ–º–∞—Ç–æ–≤ –ø–æ–ø—É–ª—è—Ä–Ω–æ–π –ø–∞—Ä—Ñ—é–º–µ—Ä–∏–∏.üå∫ *** –°–ø–µ—à–∏—Ç–µ. –î–æ –∫–æ–Ω—Ü–∞ –∞–∫—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å 3 –¥–Ω—è ***‚úàÔ∏è    \n",
      "\n",
      "Priglashaem v novij salon DOM OPTIKI na ul.Saksaganskogo,70/16! V chest' otkritija - PODARKI pokupateljam! (073)118-45-59, dom-optiki.ua\n",
      "\n",
      " \"OSCHADBANK\"  VASHU KARTKU ZABLOKOVANO.  Dlya razblokyvanya zvernicya do kontakt-centru za nom. 0919535656 goryacay liniya OSCHADBANK   \n",
      "\n",
      "The Prodigy, Kasabian, Royksopp, Onuka –∑ 28.06 –ø–æ 02.07 –Ω–∞ —Ñ–µ—Å—Ç–∏–≤–∞–ª—ñ Atlas Weekend. 8 —Å—Ü–µ–Ω —ñ 200 –≤–∏–∫–æ–Ω–∞–≤—Ü—ñ–≤! –ó—É—Å—Ç—Ä—ñ—á–∞—î–º–æ—Å—è —É –ö–∏—î–≤—ñ –Ω–∞ –í–ù–î–ì. –Ü–Ω—Ñ–æ —Ç–∞ –∫–≤–∏—Ç–∫–∏: atlasweekend.com\n",
      "\n",
      "\"Oschadbank\" Shanovnyy kliente vashu kartku  zablokovano na 92 na vashomu rahunku -0.00 UAN .Detali za nomerom: +38091-922-47-01;    \n",
      "\n",
      "–ó–∞–±–µ—Ä—ñ—Ç—å 370000 –≥—Ä–Ω –≥–æ—Ç—ñ–≤–∫–æ—é. –î–∑–≤–æ–Ω—ñ—Ç—å: 0 800 30 10 40 (–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ)\n",
      "\n",
      "–°–ø–µ—à–∏ –¥–æ 20.04.18 –≤—Å–µ–≥–æ 330–≥—Ä –∞–±–æ–Ω–µ–º–µ–Ω—Ç –ü–∞—Å—Ö–∞–ª—å–Ω—ã–π –º–µ—Å—è—Ü —Ç–µ–ª0672400404\n",
      "\n",
      "Darina, —Å–∫–∞—á–∞–π—Ç–µ –Ω–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω! booking.com/App-9nhcP09D (—Å—Å—ã–ª–∫–∞ —Å–∫–æ—Ä–æ –ø–µ—Ä–µ—Å—Ç–∞–Ω–µ—Ç –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å)\n",
      "\n",
      "–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ –í–∫—É—Å–Ω—ã—Ö –†–∞–∫–æ–≤  XS-250 S - 400.     M- 600.  –ì—Ä–Ω  L - 790 XL-980 –¢–µ–ª:0960708999 Rachevnya.com.ua.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn, fp = fnp[\"fn\"], fnp[\"fp\"]\n",
    "for el in X.iloc[fn][\"text\"]:\n",
    "    print(el+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109932</td>\n",
       "      <td>5276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636704</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean  count\n",
       "l                 \n",
       "0  0.109932   5276\n",
       "1  0.636704    534\n",
       "2  1.000000    220\n",
       "3  1.000000     63\n",
       "4  1.000000      7\n",
       "5  1.000000      1\n",
       "6  1.000000      1\n",
       "7  1.000000      1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data\n",
    " .assign(l=lambda x: x[\"text\"].str.findall(r\"%|taxi|—Å–∫–∏–¥(?:–∫|–æ—á–Ω)|—Ü[—ñ–µ]–Ω|–∑–Ω–∏–∂–∫|—Ç–∞–∫—Å[–∏—ñ]|–ø—Ä–æ–º–æ|–∞–∫—Ü[—ñ–∏]|bonus|–±–æ–Ω—É—Å\", flags=re.I|re.U).map(len))\n",
    ").groupby(\"l\")[\"label\"].agg([\"mean\", \"count\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
